## Materials and Methods

### Synthetic Lineage Data Architecture

In order to create a synthetic data to test the model, we build a tree hidden Markov model with $K$ discrete states and $N$ total number of cells. This model is composed of two primary data-structures, a hidden tree $(z_1, ..., z_N)$ and an emission tree $(x_1, ..., x_N)$. The hidden tree is a binary growth of a Markov chain with determined next hidden state based on two probability matrices, namely $\pi$ the initial probability matrix, and $T$ state-transition probability matrix. Here, $\pi$ represents the probability of starting the chain from state $z_i$, and given the current state the state-transition probability matrix $T$, determines the next state of each cell in this structure. After creating the hidden tree with the desired number of cells, the emission tree is built upon it. The fate is determined by a Bernoulli random variable which the user inputs the division rate as the parameter for each state, and the lifetime is a Gamma random variable that the user enters the shape and scale parameter $(a, s)$ for each state, respectively. After generating proper number of random variables for both observations, they are applied to each cell based on their state. This way we have a synthetic full-binary tree with assigned observations to them. Here, we have decided to generate 2 states data, representing resistant and susceptible cells to a drug. The chosen parameters and the accuracy corresponding to estimation is shown in figures...

### Tree Hidden Markov Model

One basic assumption in any Markov chain is that the next state is only dependent on the current state, and no other state; and another is that the current observation depends only on the current state. We will use these two assumptions to simplify many of the expressions employing graphical modeling tools. Proof of many of the derivations could be found in Durand et al \cite{Durand}. Assuming we have a lineage of cells in the proper format, it is aimed to use the Tree Hidden Markov model (tHMM) to predict the specific states that each cell is in, followed by estimated parameters for the distributions that the cells' observations belong to. The tHMM is composed of a number of processing functions to carry out the prediction and estimation. In this section we briefly explain the algorithms.

#### Model flow

Initial probability matrix $\pi$ is a $K \times 1$ matrix, wherein each of the elements $\pi_i$ is the probability of the root cell being in state $k$ in one lineage. In other words:

<center>$$\pi_k = P(z_1 = k), \qquad k \in \{1, ..., K\}$$</center>  
And it is clear that $\sum_{k=1}^{K} P(z_1 = k) = 1$.  
Transition probability matrix $T$ which is a $K \times K$ matrix, represents the probability of transitioning from one state to another, with respect to the Markov property; in other words every element in this square matrix is a conditional probability of the next state, given the current state:

<center>$$ T_{i,j} = T(z_i \rightarrow z_j) = P(z_j \;| z_i), \qquad i,j \in \{1, ..., K\} $$</center>

in which each row of this matrix must sum up to 1.  
The emission Likelihood matrix $EL$ is based upon the observations we have from the cells and could be more flexible. It is generally defined as the conditional probability of an observation, conditioned on being in a specific state. In our example case, we have two main observations and the sequence of each of these observations over time is the key for the tHMM model to predict the hidden states. The two observations are 1) whether a cell dies or divides, which is represented by a Bernoulli distribution with one parameter $p$, and 2) the lifetime of the cell explained by a Gamma distribution with two parameters $(a, s)$. So the Emission probability matrix here would be 2$N \times K$ matrix with elements defined as the following:

<center>$$EL(n,k) = P(x_n = x | z_n = k) $$</center>  
<center>$$ EL_b = P(x_n = x_{b_n} \;| z_n = k), \qquad k \in 1,...,K, \quad n\in \{1,...,N \} $$</center>  
<center>$$ EL_g = P(x_n = x_{g_n} \;| z_n = k) \qquad k \in 1,...,K, \quad n\in \{1,...,N\} $$</center>

in which $b_n \sim Bern(p) $ and $g_n \sim Gamma(a, s)$ in the unit of hours. In our model we assume the observations are uncorrelated, hence, the likelihood of the observations for each cell is the element-wise product of all of the likelihoods.

<center>$$ EL(n,k) = EL_b \times EL_g $$</center>  
The ultimate goal here is to find the maximum likelihood estimates of the states given the observed tree, and since the hidden states are unobserved, the best way to find the estimates is the Expectation Maximization (EM) algorithm. EM requires two steps, the expectation step in which given the whole tree, the probability of a cell and its parent being in specific states are calculated, so for every cell and every state we have $P(z_n = k \;| X_n)$ and $P(z_n = k,\; z_{n+1} = l \; | X_n)$, respectively. The E step is taken care of with the upward and downward recursion calculations \cite{something}. The second step is the M step, which is the maximizing the probability of getting the true states for each cell in the whole tree, given the observation of all the cells in the tree; in other words:  
$\max\limits_{z_1, ..., z_K}$ $P(\bar{Z} = \bar{z} | \bar{X} = \bar{x})$. The Viterbi algorithm is commonly used to find the most likely sequence of states, given the sequence of observations in a hidden Markov chain; equivalently here it returns the most likely sequence of states of the cells in a binary tree-manner by taking advantage of upward and downward recursion. Here since our data is not a simple sequence, and it is a binary tree, we follow the Viterbi algorithm introduced by Durand et al, which also has solved the problem of underflow for probabilities \cite{something}.  
Ultimately, the Baum-Welch algorithm is applied to get the maximum likelihood estimation of the parameters of the tHMM.

#### Upward and Downward Recursion

In order to avoid underflow problem to calculate these two probability matrices, Durand et al \cite{durand} proposed _upward-downward_ algorithm for smoothed probabilities in tree hidden Markov model. To explain that we would need to indtroduce the following expressions:

- $p(x)$ is noted as the parent cell of the cell $x$, and $c(n)$ is noted as children of cell $x$.
- $\bar{X}$ is the observation of the whole tree and $\bar{X}_a$ is a subtree of $\bar{X}$ which is rooted at cell $a$. Also, $\bar{Z}$ is the complete hidden state tree.
- $\bar{X}_{a/b}$ is the subtree rooted at $a$ except for the subtree rooted at cell $b$, if $\bar{X}_b$ is a subtree of $\bar{X}_a$.

For state prediction in tHMM model we start with calculating a few matrices:  
Marginal state distribution (MSD) is an $N \times K$ matrix that for each cell is marginalizing the transition probability over all possible current states by downward traversing from root cells to leaf cells:

<center>$$ MSD(n,k) = P(z_{n} = k)= \sum_{i} P(z_n = k |z_{n-1} = i)\times P(z_{n-1} = i)$$</center>

**Upward recursion:**  
The flow of upward probabilities is from leaf cells to the root cells generation by generation. For leaf cells, which are the base case to start with the $\beta$s are calculated by:

<center>$$\beta(n,k) = P(z_n = k\;|X_n = x_n) = \frac{EL(n,k) \times MSD(n,k)}{NF_l(n)}$$</center>  
in which $X$ is the leaf cell's observation, and NF (Normalizing Factor) is an $N \times 1$ matrix that is the marginal observation distribution. Since $\sum_{k} \beta_n(k) = 1$, we find the expression for NF for leaf cells as:  
<center>$$NF_l(n) = \sum_{k} EL(n,k) \times MSD(n,k) = P(X_n = x_n)$$</center>  
For non-leaf cells the upward values are given by:  
<center>$$ \beta(n,k) = P(z_n = k\;|\bar{X}_n = \bar{x}_n) = \frac{EL(n,k) \times MSD(n,k) \times \prod_{v \in c(n)}\beta_{n,v}(k)}{NF_{nl}(n)}$$</center>  
in which we can extract non-leaf NF such that:  
<center>$$ NF_{nl}(n) = \sum_{k} \Big[EL(n,k) \times MSD(n,k) \prod_{v \in c(n)} \beta_{n,v}(k)\Big] $$</center>  
and linking $\beta$ between parent-daughter cells is given by:  
<center>$$ \beta_{p(n), n}(k) = P(\bar{X}_n = \bar{x}_n | z_{p(n)} = j) = \sum_{j} \frac{\beta_n(j) \times T_{k,j}}{MSD(n,j)}$$</center>

By recursing from leaf cells to root cells, $\beta$ and NF matrix are calculated as upward recursion. Calculating the NF matrix gives a convenient expression for the log-likelihood of the observations. For the root cell we have:

<center>$$ P(\bar{X} = \bar{x}) = \prod_{n} \frac{P(\bar{X}_n = \bar{x}_n)}{\prod_{v\in c(n)} P(\bar{X}_v = \bar{x}_v)} = \sum_{n} NF(n) \qquad n \in \{1, ..., N\}$$</center>  
As a conclusion:  
<center>$$ log P(\bar{X} = \bar{x}) = \sum_{n} log NF(n)$$</center>  
This helps with keeping track of the log-likelihood which is a measure of convergence of the EM algorithm.

**Downward recursion:**  
For computing downward recursion, we need the following definition starting from the root cells:

<center>$$ \gamma_1(k) = P(z_1 = k | \bar{X}_1 = \bar{x}_1) = \beta_1(k)$$</center>  
So it follows for other cells as:  
<center>$$ \gamma_n(k) = P(z_n = k | \bar{X}_1 = \bar{x}_1) = \frac{\beta_n(k)}{MSD(n,k)} \sum_{i}\frac{T_{i,k} \gamma_{p(n)}(i)}{\beta_{p(n),n}(i)}$$</center>  
that is an $N \times K$ matrix, achieved by writing the conditional probabilities as the summation over the joint probabilities of parent-daughter cells.

#### Viterbi Algorithm

In Hidden Markov Models, in order to find the most likely sequence of hidden states given a sequence of observations, Viterbi algorithm is employed. Here, we intend to find the most likely tree of states given the tree of observations, in other words $\max\limits_{z_1, z_2,...,z_K}P(\bar{Z} = \bar{z} , \bar{X} = \bar{x})$. Adapted Viterbi follows from a upward recursion from leaf cells to the root, meaning we first define the values for the leaf cells and move to parent cells up to the root, and we implement the algorithm developed in Durand et al.  
We define $\delta$, an $N \times K$ matrix:

<center>$$ \delta (n,k) = \max\limits_{\bar{z}_{c(n)}}\{P(\bar{X}_n = \bar{x}_n, \bar{Z}_{c(n)} = \bar{z}_{c(n)} | z_n = j)\} $$</center>  
and the links between parent-daughter cells as:  
<center>$$ \delta_{p(n),n}(k) = \max\limits_{\bar{z}_n} \{P(\bar{X}_n = \bar{x}_n, \bar{Z}_n = \bar{z}_n | z_{p(n)} = j)\} = \max\limits_{j}\{\delta(n,j) T_{k,j}\}$$</center>  
So we initialize from the leaf cells as:  
<center>$$ \delta(n,k) = P(X_n = x_n | z_n = k) = EL(n,k)$$</center>  
and for non-leaf cells we have:  
<center>$$ \delta(n,k) =  \Big[\prod_{v \in c(n)} \delta_{n,v}(k)\Big]\times EL(n,k)$$</center>  
The probability of the optimal state tree corresponding to the observations tree assuming root cell is noted as cell 1, is given by:
<center>$$ Z^* = \max\limits_{k}\{\delta(1,k) \pi_k \}$$</center>  
which is resulted from writing the maximization over conditional emission likelihood (EL) probabilities by factoring out the root cells as the outer maximizing step over all possible states. In this step, $Z^*$ which is the hidden state tree is calculated. After estimating the most likely state for each cell, we group the cell based on their state, which will then be used to estimate the distribution parameters.

#### Baum-Welch Algorithm

In order to estimate the parameters corresponding to the hidden Markov models, Baum-Welch algorithm is used which employs the EM algorithm to find the maximum likelihood of the parameters. Here, we estimate $\theta = (\pi, T, p, a, s)$ the initial and transition probability matrices, the parameters of the observation distributions, namely Bernoulli parameter, shape, and scale parameter of Gamma distribution by maximizing the probability of the observations given the parameters; in other words $\theta^* = \max\limits_{\theta} P(X, Z|\theta)$, that is maximizing the joint probabilities.

<center>$$ \xi_n(j,k) = P(z_n = j, z_{p(n)} = i | \bar{X}_1 = \bar{x}_1)$$</center>  
According to Bayes theorem, we can write the above probability in a joint form, and since $P(\bar{X}_1) = \bar{x}_1$ is the EL probabilities of the whole tree, it is just a constant coefficient and we can ignore it.  
So we could write it as:  
<center>$$ \xi_n(j,k) = \frac{\beta(n+1, k) \times T_{j,k} \times \gamma_j(n)}{MSD(n+1, k) \times \beta_{p(n),n}(j)}$$</center>  
The maximum likelihood estimation of the initial probabilities:  
<center>$$ \pi^*_k = \gamma_1(k)$$</center>  
The transition probability matrix would be estimated as:  
<center>$$ T^*_{i,j} = \frac{\sum_{n=1}^{N-1} \xi_n(i,j)}{\sum_{n=1}^{N-1} \gamma_n(i)} $$</center>

To estimate the distribution parameters, after finding the most likely state for each cell, we group them and group their observations based on their estimated state, and pass them to _StateDistribution_ class for maximum likelihood estimation. For estimating the Bernoulli parameter the estimated parameter is simply the sample mean of the observations for each state. Assuming we have $N_1$ data points estimated in state 1, the corresponding Bernoulli parameter for state 1 would be:

<center>$$ p^*_1 = \frac{\sum_{i=1}^{N_1} x_b(i)}{N_1}$$</center>  
And for Gamma distribution parameters we use a closed-form estimation based on \cite{gamma_estimation} which has been corrected for bias:  
<center>$$ a^* = \frac{N_1 \times  \sum_{i=1}^{N_1} x_g(i)}{\Big[N_1 \times \sum_{i=1}^{N_1} x_g(i) ln(x_g(i))\Big] - \sum_{i=1}^{N_1} ln(x_g(i)) \times \sum_{i=1}^{N_1} x_g(i)} $$</center>  
<center>$$ s^* = \frac{1}{N_1^2} \times \Big[N_1 \times \sum_{i=1}^{N_1} x_g(i) ln(x_g(i)) - \sum_{i=1}^{N_1} ln(x_g(i)) \times \sum_{i=1}^{N_1} x_g(i) \Big]$$</center>

### Experimental single-cell lineage data

The data includes AU565 breast cancer cell line for control condition along with 7 concentrations of 4 types of targeted or chemotherapy compounds including Lapatinib, Gemcitabine, Paclitaxel, and Doxorubicin. A fluorescent reporter translocates between the nucleus and cytoplasm indicating the phase of the cells. Each cell is indicated as to be in G1 or S/G2 phase according to the location of the reporter. When the reporter is located in the nucleus, the cells is in G1 and when the reporter in in the cytoplasm, the cell is passing through S/G2 phase. Each experiment lasts for 96 hours where the plates were being imaged every 30 minutes. The experiments were repeated three times. Single cells were tracked using CellProfiler Analyst software (or manual tracking?) To collect cell fate in either phase or the amount of time it takes for each cell to pass through G1 and S/G2 phase.

#### Strategies to overcome model fitting to cernsored data

In single-cell experimental data collection, there lineages are almost never fully observed, in other word we experience missing data for cells with time or fate censorship. For those leaf cells living at the end of the collected lineages, which may have been in their G1 or S/G2 exactly when the experiment was finished, experience a right-censorship, meaning, we have missing data from the future. For a cell that was in G1 at the end of the experiment, the time that the cell transitions to S/G2 is unknown, hence the cell's G1 is censored. In a similar way, almost all of the root cells appearing at the very beginning of the experiment are left-censored. For instance, if the cell is in S/G2 phase at the very beginning of the experiment, its G1 duration is a censored value, since the time the cell was born is unknoun, missing data from the past.  
To deal with the censorship, we have employed estimator of the censored dustribution. This way, given a population of cells, we identify whether any of the cell's observation are censored or not, then censored values will be fed into the censored estimator, and those that have been fully observed, will be given to the common uncensored estimator.  
Another similar challenge is the unobserved measurements. This happens when we have a leaf cell that is in its G1 when the experiment is finished. This way, we would not have any knowledge of the cell's S/G2 phase. This could happen at the start or end of the experiment. We simply remove those measurements that have not been observed at all.

### Computational Tools to explore the model

In this part we briefly mention the measures we used to monitor the goodness of the fit of the model.  
**AIC:**  
In order to find the most likely number of states corresponding to the observations, Akaike Information Criterion (AIC) is used. This could be considered as an approach for model selection that could inform us of the most likely number of states, and also it deals with the trade off between over-fitting and under-fitting [@doi:10.1006/jmps.1999.1276].

**Wasserstein distance:**  
Wasserstein or Kantorovichâ€“Rubinstein metric, is a measure of distance between two distributions. In order to show how different the states should be to be recognized by the model, we have employed Wasserstein distance [@doi:10.1137/1118101].

### Figure Generation

**Figure 7 Parameters**:

<style>

    #fig7param {
    border-collapse: collapse;
    }

    #fig7param td {
        text-align: center;
    }

    #fig7param th, #fig7param td {
    padding: 5px;
    height:20px;
    width: 100px;
    }
    #fig7param th{
        border-bottom: 1px solid black;
        background-color: #f2f2f2;
    }
    #fig7param td.row{
        text-align: left;
    }

    #fig7param tr:nth-child(odd){background-color: #f2f2f2}   
</style>

<table id = "fig7param">
    <tr>
        <th> State </th>
        <th> Bernoulli G<sub>1</sub> </th>
        <th> Bernoulli G<sub>2</sub> </th>
        <th> Shape G<sub>1</sub> </th>
        <th> Scale G<sub>1</sub> </th>
        <th> Shape G<sub>2</sub> </th>
        <th> Scale G<sub>2</sub> </th>    
    </tr>
    <tr>
        <td class = "row"> State 1 </td>
        <td> 0.99 </td>
        <td> 0.9 </td>
        <td> 10 </td>
        <td> 2 </td>
        <td> 10 </td>
        <td> 2 </td>
    </tr>
    <tr>
        <td class = "row"> State 2 </td>
        <td> 0.9 </td>
        <td> 0.9 </td>
        <td> 20 </td>
        <td> 3 </td>
        <td> 20 </td>
        <td> 3 </td>
    </tr>
    <tr>
        <td class = "row"> State 3 </td>
        <td> 0.85 </td>
        <td> 0.9 </td>
        <td> 30 </td>
        <td> 4 </td>
        <td> 30 </td>
        <td> 4 </td>
    </tr>
    <tr>
        <td class = "row"> State 4 </td>
        <td> 0.8 </td>
        <td> 0.9 </td>
        <td> 40</td>
        <td> 4 </td>
        <td> 40 </td>
        <td> 5 </td>
    </tr>
</table>


