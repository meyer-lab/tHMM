## Materials and Methods

### Synthetic Lineage Data Architecture  
In order to create a synthetic data to test the model, we build a tree hidden Markov model with $K$ discrete states and $N$ total number of cells. This model is composed of two primary data-structures, a hidden tree $(z_1, ..., z_N)$ and an emission tree $(x_1, ..., x_N)$. The hidden tree is a binary growth of a Markov chain with determined next hidden state based on two probability matrices, namely $\pi$ the initial probability matrix, and $T$ state-transition probability matrix. Here, $\pi$ represents the probability of starting the chain from state $z_i$, and given the current state the state-transition probability matrix $T$, determines the next state of each cell in this structure. After creating the hidden tree with the desired number of cells, the emission tree is built upon it. The fate is determined by a Bernoulli random variable which the user inputs the division rate as the parameter for each state, and the lifetime is a Gamma random variable that the user enters the shape and scale parameter $(a, s)$ for each state, respectively. After generating proper number of random variables for both observations, they are applied to each cell based on their state. This way we have a synthetic full-binary tree with assigned observations to them. Here, we have decided to generate 2 states data, representing resistant and susceptible cells to a drug. The chosen parameters and the accuracy corresponding to estimation is shown in figures...

### Tree Hidden Markov Model  
One basic assumption in any Markov chain is that the next state is only dependent on the current state, and no other state; and another is that the current observation depends only on the current state. We will use these two assumptions to simplify many of the expressions employing graphical modeling tools. Proof of many of the derivations could be found in Durand et al \cite{Durand}. Assuming we have a lineage of cells in the proper format, it is aimed to use the Tree Hidden Markov model (tHMM) to predict the specific states that each cell is in, followed by estimated parameters for the distributions that the cells' observations belong to. The tHMM is composed of a number of processing functions to carry out the prediction and estimation. In this section we briefly explain the algorithms.  

#### Model flow  
Initial probability matrix $\pi$ is a $K \times 1$ matrix, wherein each of the elements $\pi_i$ is the probability of the root cell being in state $k$ in one lineage. In other words:  
<center>$$\pi_k = P(z_1 = k), \qquad k \in \{1, ..., K\}$$</center>  
And it is clear that $\sum_{k=1}^{K} P(z_1 = k) = 1$.  
Transition probability matrix $T$ which is a $K \times K$ matrix, represents the probability of transitioning from one state to another, with respect to the Markov property; in other words every element in this square matrix is a conditional probability of the next state, given the current state:  

<center>$$ T_{i,j} = T(z_i \rightarrow z_j) = P(z_j \;| z_i), \qquad i,j \in \{1, ..., K\} $$</center>  

in which each row of this  matrix must sum up to 1.  
The emission Likelihood matrix $EL$ is based upon the observations we have from the cells and could be more flexible. It is generally defined as the conditional probability of an observation, conditioned on being in a specific state. In our example case, we have two main observations and the sequence of each of these observations over time is the key for the tHMM model to predict the hidden states. The two observations are 1) whether a cell dies or divides, which is represented by a Bernoulli distribution with one parameter $p$, and 2) the lifetime of the cell explained by a Gamma distribution with two parameters $(a, s)$. So the Emission probability matrix here would be 2$N \times K$ matrix with elements defined as the following:  
<center>$$EL(n,k) = P(x_n = x | z_n = k) $$</center>  
<center>$$ EL_b = P(x_n = x_{b_n} \;| z_n = k), \qquad k \in 1,...,K, \quad n\in \{1,...,N \} $$</center>  
<center>$$ EL_g = P(x_n = x_{g_n} \;| z_n = k) \qquad k \in 1,...,K, \quad n\in \{1,...,N\} $$</center>  

in which $b_n \sim Bern(p) $ and $g_n \sim Gamma(a, s)$ in the unit of hours. In our model we assume the observations are uncorrelated, hence, the likelihood of the observations for each cell is the element-wise product of all of the likelihoods.  
<center>$$ EL(n,k) = EL_b \times EL_g $$</center>  
The ultimate goal here is to find the maximum likelihood estimates of the states given the observed tree, and since the hidden states are unobserved, the best way to find the estimates is the Expectation Maximization (EM) algorithm. EM requires two steps, the expectation step in which given the whole tree, the probability of a cell and its parent being in specific states are calculated, so for every cell and every state we have $P(z_n = k \;| X_n)$ and $P(z_n = k,\; z_{n+1} = l \; | X_n)$, respectively. The E step is taken care of with the upward and downward recursion calculations \cite{something}. The second step is the M step, which is the maximizing the probability of getting the true states for each cell in the whole tree, given the observation of all the cells in the tree; in other words:  
$\max\limits_{z_1, ..., z_K}$ $P(\bar{Z} = \bar{z} | \bar{X} = \bar{x})$. The Viterbi algorithm is commonly used to find the most likely sequence of states, given the sequence of observations in a hidden Markov chain; equivalently here it returns the most likely sequence of states of the cells in a binary tree-manner by taking advantage of upward and downward recursion. Here since our data is not a simple sequence, and it is a binary tree, we follow the Viterbi algorithm introduced by Durand et al, which also has solved the problem of underflow for probabilities \cite{something}.  
Ultimately, the Baum-Welch algorithm is applied to get the maximum likelihood estimation of the parameters of the tHMM.  

#### Upward and Downward Recursion  
In order to avoid underflow problem to calculate these two probability matrices, Durand et al \cite{durand} proposed *upward-downward* algorithm for smoothed probabilities in tree hidden Markov model. To explain that we would need to indtroduce the following expressions:  

- $p(x)$ is noted as the parent cell of the cell $x$, and $c(n)$ is noted as children of cell $x$.
- $\bar{X}$ is the observation of the whole tree and $\bar{X}_a$ is a subtree of $\bar{X}$ which is rooted at cell $a$. Also, $\bar{Z}$ is the complete hidden state tree.
- $\bar{X}_{a/b}$ is the subtree rooted at $a$ except for the subtree rooted at cell $b$, if $\bar{X}_b$ is a subtree of $\bar{X}_a$. 

For state prediction in tHMM model we start with calculating a few matrices:  
Marginal state distribution (MSD) is an $N \times K$ matrix that for each cell is marginalizing the transition probability over all possible current states by downward traversing from root cells to leaf cells:  
<center>$$ MSD(n,k) = P(z_{n} = k)= \sum_{i} P(z_n = k |z_{n-1} = i)\times P(z_{n-1} = i)$$</center>  

__Upward recursion:__  
The flow of upward probabilities is from leaf cells to the root cells generation by generation. For leaf cells, which are the base case to start with the $\beta$s are calculated by:  
<center>$$\beta(n,k) = P(z_n = k\;|X_n = x_n) = \frac{EL(n,k) \times MSD(n,k)}{NF_l(n)}$$</center>  
in which $X$ is the leaf cell's observation, and NF (Normalizing Factor) is an $N \times 1$ matrix that is the marginal observation distribution. Since $\sum_{k} \beta_n(k) = 1$, we find the expression for NF for leaf cells as:  
<center>$$NF_l(n) = \sum_{k} EL(n,k) \times MSD(n,k) = P(X_n = x_n)$$</center>  
For non-leaf cells the upward values are given by:  
<center>$$ \beta(n,k) = P(z_n = k\;|\bar{X}_n = \bar{x}_n) = \frac{EL(n,k) \times MSD(n,k) \times \prod_{v \in c(n)}\beta_{n,v}(k)}{NF_{nl}(n)}$$</center>  
in which we can extract non-leaf NF such that:  
<center>$$ NF_{nl}(n) = \sum_{k} \Big[EL(n,k) \times MSD(n,k) \prod_{v \in c(n)} \beta_{n,v}(k)\Big] $$</center>  
and linking $\beta$ between parent-daughter cells is given by:  
<center>$$ \beta_{p(n), n}(k) = P(\bar{X}_n = \bar{x}_n | z_{p(n)} = j) = \sum_{j} \frac{\beta_n(j) \times T_{k,j}}{MSD(n,j)}$$</center>  

By recursing from leaf cells to root cells, $\beta$ and NF matrix are calculated as upward recursion. Calculating the NF matrix gives a convenient expression for the log-likelihood of the observations. For the root cell we have:  
<center>$$ P(\bar{X} = \bar{x}) = \prod_{n} \frac{P(\bar{X}_n = \bar{x}_n)}{\prod_{v\in c(n)} P(\bar{X}_v = \bar{x}_v)} = \sum_{n} NF(n) \qquad n \in \{1, ..., N\}$$</center>  
As a conclusion:  
<center>$$ log P(\bar{X} = \bar{x}) = \sum_{n} log NF(n)$$</center>  
This helps with keeping track of the log-likelihood which is a measure of convergence of the EM algorithm.  

__Downward recursion:__  
For computing downward recursion, we need the following definition starting from the root cells:  
<center>$$ \gamma_1(k) = P(z_1 = k | \bar{X}_1 = \bar{x}_1) = \beta_1(k)$$</center>  
So it follows for other cells as:  
<center>$$ \gamma_n(k) = P(z_n = k | \bar{X}_1 = \bar{x}_1) = \frac{\beta_n(k)}{MSD(n,k)} \sum_{i}\frac{T_{i,k} \gamma_{p(n)}(i)}{\beta_{p(n),n}(i)}$$</center>  
that is an $N \times K$ matrix, achieved by writing the conditional probabilities as the summation over the joint probabilities of parent-daughter cells.  

#### Viterbi Algorithm  
In Hidden Markov Models, in order to find the most likely sequence of hidden states given a sequence of observations, Viterbi algorithm is employed. Here, we intend to find the most likely tree of states given the tree of observations, in other words $\max\limits_{z_1, z_2,...,z_K}P(\bar{Z} = \bar{z} , \bar{X} = \bar{x})$. Adapted Viterbi follows from a upward recursion from leaf cells to the root, meaning we first define the values for the leaf cells and move to parent cells up to the root, and we implement the algorithm developed in Durand et al.  
We define $\delta$, an $N \times K$ matrix:  
<center>$$ \delta (n,k) = \max\limits_{\bar{z}_{c(n)}}\{P(\bar{X}_n = \bar{x}_n, \bar{Z}_{c(n)} = \bar{z}_{c(n)} | z_n = j)\} $$</center>  
and the links between parent-daughter cells as:  
<center>$$ \delta_{p(n),n}(k) = \max\limits_{\bar{z}_n} \{P(\bar{X}_n = \bar{x}_n, \bar{Z}_n = \bar{z}_n | z_{p(n)} = j)\} = \max\limits_{j}\{\delta(n,j) T_{k,j}\}$$</center>  
So we initialize from the leaf cells as:  
<center>$$ \delta(n,k) = P(X_n = x_n | z_n = k) = EL(n,k)$$</center>  
and for non-leaf cells we have:  
<center>$$ \delta(n,k) =  \Big[\prod_{v \in c(n)} \delta_{n,v}(k)\Big]\times EL(n,k)$$</center>  
The probability of the optimal state tree corresponding to the observations tree assuming root cell is noted as cell 1, is given by:
<center>$$ Z^* = \max\limits_{k}\{\delta(1,k) \pi_k \}$$</center>  
which is resulted from writing the maximization over conditional emission likelihood (EL) probabilities by factoring out the root cells as the outer maximizing step over all possible states. In this step, $Z^*$ which is the hidden state tree is calculated. After estimating the most likely state for each cell, we group the cell based on their state, which will then be used to estimate the distribution parameters.  

#### Baum-Welch Algorithm  
In order to estimate the parameters corresponding to the hidden Markov models, Baum-Welch algorithm is used which employs the EM algorithm to find the maximum likelihood of the parameters. Here, we estimate $\theta = (\pi, T, p, a, s)$ the initial and transition probability matrices, the parameters of the observation distributions, namely Bernoulli parameter, shape, and scale parameter of Gamma distribution by maximizing the probability of the observations given the parameters; in other words $\theta^* = \max\limits_{\theta} P(X, Z|\theta)$, that is maximizing the joint probabilities.  
<center>$$ \xi_n(j,k) = P(z_n = j, z_{p(n)} = i | \bar{X}_1 = \bar{x}_1)$$</center>  
According to Bayes theorem, we can write the above probability in a joint form, and since $P(\bar{X}_1) = \bar{x}_1$ is the EL probabilities of the whole tree, it is just a constant coefficient and we can ignore it.  
So we could write it as:  
<center>$$ \xi_n(j,k) = \frac{\beta(n+1, k) \times T_{j,k} \times \gamma_j(n)}{MSD(n+1, k) \times \beta_{p(n),n}(j)}$$</center>  
The maximum likelihood estimation of the initial probabilities:  
<center>$$ \pi^*_k = \gamma_1(k)$$</center>  
The transition probability matrix would be estimated as:  
<center>$$ T^*_{i,j} = \frac{\sum_{n=1}^{N-1} \xi_n(i,j)}{\sum_{n=1}^{N-1} \gamma_n(i)} $$</center>  

To estimate the distribution parameters, after finding the most likely state for each cell, we group them and group their observations based on their estimated state, and pass them to *StateDistribution* class for maximum likelihood estimation. For estimating the Bernoulli parameter the estimated parameter is simply the sample mean of the observations for each state. Assuming we have $N_1$ data points estimated in state 1, the corresponding Bernoulli parameter for state 1 would be:
<center>$$ p^*_1 = \frac{\sum_{i=1}^{N_1} x_b(i)}{N_1}$$</center>  
And for Gamma distribution parameters we use a closed-form estimation based on \cite{gamma_estimation} which has been corrected for bias:  
<center>$$ a^* = \frac{N_1 \times  \sum_{i=1}^{N_1} x_g(i)}{\Big[N_1 \times \sum_{i=1}^{N_1} x_g(i) ln(x_g(i))\Big] - \sum_{i=1}^{N_1} ln(x_g(i)) \times \sum_{i=1}^{N_1} x_g(i)} $$</center>  
<center>$$ s^* = \frac{1}{N_1^2} \times \Big[N_1 \times \sum_{i=1}^{N_1} x_g(i) ln(x_g(i)) - \sum_{i=1}^{N_1} ln(x_g(i)) \times \sum_{i=1}^{N_1} x_g(i) \Big]$$</center>  

### Experimental Lineage Data  
TBD  

### Computational Tools to explore the model  
In this part we briefly mention the measures we used to monitor the goodness of the fit of the model.  
__AIC:__  
In order to find the most likely number of states corresponding to the observations, Akaike Information Criterion (AIC) is used. This could be considered as an approach for model selection that could inform us of what value of $K$ is the best, and also it deals with the trade off between over-fitting and under-fitting. AIC is calculated using the following expression, where NLL is the negative log-likelihood, and P is the number of parameters:  
<center>$$ AIC = 2(NLL + K^2 + KP - 1)$$</center>  

__KL-divergence:__  
Kullback-Leibler divergence (KL-divergence) is a measure of difference between the two distributions. Here we want to show if the two states of the cells in the data, are far from each other, meaning the cells in the two states are more different, then the model is better at distinguishing the two states, hence the accuracy of parameter estimation and state assignment is higher.  
For that, we use the entropy, a built-in function in Python Scipy package. By definition the KL-divergence for two probability distributions $P$ and $Q$ is:  
<center>$$ D_{KL}(P || Q) = \sum P(x) log\Big(\frac{P(x)}{Q(x)}\Big)$$</center>  
