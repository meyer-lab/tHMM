## Materials and Methods

### Synthetic lineage data architecture

In order to create a synthetic data, we build a lineage tree with $K$ discrete states and $N$ total number of cells. This model is composed of two primary data-structures, a hidden tree $(z_1, ..., z_N)$ and an emission tree $(x_1, ..., x_N)$. The hidden tree is a binary growth of a Markov chain with determined next hidden state based on two probability matrices, namely $\pi$ the initial probability matrix, and $T$ state-transition probability matrix. Here, $\pi$ represents the probability of starting the chain from any of the states, and given the current state the state-transition probability matrix $T$, determines the next state of each cell in this structure. After creating the hidden tree with the desired number of cells, the emission tree is built upon it. The fate is determined by a Bernoulli random variable which the user inputs the division rate as the parameter for each state, and the lifetime is a Gamma random variable that the user enters the shape and scale parameter $(a, s)$ for each state, respectively. After generating the proper number of random variables for both observations, they are applied to each cell based on their state. This way we have a synthetic binary tree with assigned observations to them. In the case of phase-specific observations, we need a set of Bernoulli and Gamma distribution parameters for each phase, separately. Here, we benchmark our model for a 2-state population for both phase specific and phase non-specific scenarios.

### Tree hidden Markov model

One basic assumption in any Markov chain is that the next state is only dependent on the current state, and no other state; and another is that the current observation depends only on the current state. We will use these two assumptions to simplify many of the expressions employing graphical modeling tools. Proof of many of the derivations could be found in Durand et al. [@doi:10.1109/TSP.2004.832006]. Assuming we have a lineage of cells with known observations, it is aimed to use the tree hidden Markov model (tHMM) to predict the specific states that each cell is in, followed by estimated parameters for the distributions that the cells' observations belong to. The tHMM is composed of a number of processing functions to carry out the prediction and estimation. In this section we briefly explain the algorithms.

#### Model flow

Initial probability matrix $\pi$ is a $K \times 1$ matrix, wherein each of the elements $\pi_i$ is the probability of the root cell being in state $k$ in one lineage. In other words:

<center>$$\pi_k = P(z_1 = k), \qquad k \in \{1, ..., K\}$$</center>  
And it is clear that $\sum_{k=1}^{K} P(z_1 = k) = 1$.  
Transition probability matrix $T$ which is a $K \times K$ matrix, represents the probability of transitioning from one state to another, with respect to the Markov property; in other words every element in this square matrix is a conditional probability of the next state, given the current state:

<center>$$ T_{i,j} = T(z_i \rightarrow z_j) = P(z_j \;| z_i), \qquad i,j \in \{1, ..., K\} $$</center>

in which each row of this matrix must sum up to 1.  
The emission Likelihood matrix $EL$ is based upon the observations we have from the cells and could be more flexible. It is generally defined as the conditional probability of an observation, conditioned on being in a specific state. In our example case, we have two main observations and the sequence of each of these observations over time is the key for the tHMM model to predict the hidden states. The two observations are 1) whether a cell dies or divides, which is represented by a Bernoulli distribution with one parameter $p$, and 2) the lifetime of the cell explained by a Gamma distribution with two parameters $(a, s)$. So the Emission probability matrix here would be 2$N \times K$ matrix with elements defined as the following:

<center>$$EL(n,k) = P(x_n = x | z_n = k) $$</center>  
<center>$$ EL_b = P(x_n = x_{b_n} \;| z_n = k), \qquad k \in 1,...,K, \quad n\in \{1,...,N \} $$</center>  
<center>$$ EL_g = P(x_n = x_{g_n} \;| z_n = k) \qquad k \in 1,...,K, \quad n\in \{1,...,N\} $$</center>

in which $b_n \sim Bern(p)$ and $g_n \sim Gamma(a, s)$ in the unit of hours. In our model we assume the observations are uncorrelated, hence, the likelihood of the observations for each cell is the element-wise product of all of the likelihoods.

<center>$$ EL(n,k) = EL_b \times EL_g $$</center>  
The ultimate goal here is to find the maximum likelihood estimates of the states given the observed tree, and since the hidden states are unobserved, the best way to find the estimates is the Expectation Maximization (EM) algorithm. EM requires two steps, the expectation step in which given the whole tree, the probability of a cell and its parent being in specific states are calculated, so for every cell and every state we have $P(z_n = k \;| X_n)$ and $P(z_n = k,\; z_{n+1} = l \; | X_n)$, respectively. The E step is taken care of with the upward and downward recursion calculations \cite{something}. The second step is the M step, which is maximizing the probability of getting the true states for each cell in the whole tree, given the observation of all the cells in the tree; in other words:  
$\max\limits_{z_1, ..., z_K}$ $P(\bar{Z} = \bar{z} | \bar{X} = \bar{x})$. The Viterbi algorithm is commonly used to find the most likely sequence of states, given the sequence of observations in a hidden Markov chain; equivalently, here it returns the most likely sequence of states of the cells in a binary tree-manner by taking advantage of upward and downward recursion. Here, since our data is not a simple sequence, and it is a binary tree, we follow the Viterbi algorithm introduced by Durand et al, which also has solved the problem of underflow for probabilities \cite{something}.  
Ultimately, the Baum-Welch algorithm is applied to get the maximum likelihood estimation of the parameters of the tHMM.

#### Upward and downward recursion

In order to avoid underflow problem to calculate these two probability matrices, Durand et al \cite{durand} proposed _upward-downward_ algorithm for smoothed probabilities in tree hidden Markov model. To explain that we would need to introduce the following expressions:

- $p(x)$ is noted as the parent cell of the cell $x$, and $c(n)$ is noted as children of cell $x$.
- $\bar{X}$ is the observation of the whole tree and $\bar{X}_a$ is a subtree of $\bar{X}$ which is rooted at cell $a$. Also, $\bar{Z}$ is the complete hidden state tree.
- $\bar{X}_{a/b}$ is the subtree rooted at $a$ except for the subtree rooted at cell $b$, if $\bar{X}_b$ is a subtree of $\bar{X}_a$.

For state prediction in tHMM model we start with calculating a few matrices:  
Marginal state distribution (MSD) is an $N \times K$ matrix that for each cell is marginalizing the transition probability over all possible current states by downward traversing from root cells to leaf cells:

<center>$$ MSD(n,k) = P(z_{n} = k)= \sum_{i} P(z_n = k |z_{n-1} = i)\times P(z_{n-1} = i)$$</center>

**Upward recursion:**  
The flow of upward probabilities is from leaf cells to the root cells generation by generation. For leaf cells, which are the base case to start with the $\beta$s are calculated by:

<center>$$\beta(n,k) = P(z_n = k\;|X_n = x_n) = \frac{EL(n,k) \times MSD(n,k)}{NF_l(n)}$$</center>  
in which $X$ is the leaf cell's observation, and NF (Normalizing Factor) is an $N \times 1$ matrix that is the marginal observation distribution. Since $\sum_{k} \beta_n(k) = 1$, we find the expression for NF for leaf cells as:  
<center>$$NF_l(n) = \sum_{k} EL(n,k) \times MSD(n,k) = P(X_n = x_n)$$</center>  
For non-leaf cells the upward values are given by:  
<center>$$ \beta(n,k) = P(z_n = k\;|\bar{X}_n = \bar{x}_n) = \frac{EL(n,k) \times MSD(n,k) \times \prod_{v \in c(n)}\beta_{n,v}(k)}{NF_{nl}(n)}$$</center>  
in which we can extract non-leaf NF such that:  
<center>$$ NF_{nl}(n) = \sum_{k} \Big[EL(n,k) \times MSD(n,k) \prod_{v \in c(n)} \beta_{n,v}(k)\Big] $$</center>  
and linking $\beta$ between parent-daughter cells is given by:  
<center>$$ \beta_{p(n), n}(k) = P(\bar{X}_n = \bar{x}_n | z_{p(n)} = j) = \sum_{j} \frac{\beta_n(j) \times T_{k,j}}{MSD(n,j)}$$</center>

By recursing from leaf cells to root cells, $\beta$ and NF matrix are calculated as upward recursion. Calculating the NF matrix gives a convenient expression for the log-likelihood of the observations. For the root cell we have:

<center>$$ P(\bar{X} = \bar{x}) = \prod_{n} \frac{P(\bar{X}_n = \bar{x}_n)}{\prod_{v\in c(n)} P(\bar{X}_v = \bar{x}_v)} = \sum_{n} NF(n) \qquad n \in \{1, ..., N\}$$</center>  
As a conclusion:  
<center>$$ log P(\bar{X} = \bar{x}) = \sum_{n} log NF(n)$$</center>  
This helps with keeping track of the log-likelihood which is a measure of convergence of the EM algorithm.

**Downward recursion:**  
For computing downward recursion, we need the following definition starting from the root cells:

<center>$$ \gamma_1(k) = P(z_1 = k | \bar{X}_1 = \bar{x}_1) = \beta_1(k)$$</center>  
So it follows for other cells as:  
<center>$$ \gamma_n(k) = P(z_n = k | \bar{X}_1 = \bar{x}_1) = \frac{\beta_n(k)}{MSD(n,k)} \sum_{i}\frac{T_{i,k} \gamma_{p(n)}(i)}{\beta_{p(n),n}(i)}$$</center>  
that is an $N \times K$ matrix, achieved by writing the conditional probabilities as the summation over the joint probabilities of parent-daughter cells.

#### Viterbi algorithm

In Hidden Markov Models, in order to find the most likely sequence of hidden states given a sequence of observations, Viterbi algorithm is employed. Here, we intend to find the most likely tree of states given the tree of observations, in other words $\max\limits_{z_1, z_2,...,z_K}P(\bar{Z} = \bar{z} , \bar{X} = \bar{x})$. Adapted Viterbi follows from a upward recursion from leaf cells to the root, meaning we first define the values for the leaf cells and move to parent cells up to the root, and we implement the algorithm developed in Durand et al.  
We define $\delta$, an $N \times K$ matrix:

<center>$$ \delta (n,k) = \max\limits_{\bar{z}_{c(n)}}\{P(\bar{X}_n = \bar{x}_n, \bar{Z}_{c(n)} = \bar{z}_{c(n)} | z_n = j)\} $$</center>  
and the links between parent-daughter cells as:  
<center>$$ \delta_{p(n),n}(k) = \max\limits_{\bar{z}_n} \{P(\bar{X}_n = \bar{x}_n, \bar{Z}_n = \bar{z}_n | z_{p(n)} = j)\} = \max\limits_{j}\{\delta(n,j) T_{k,j}\}$$</center>  
So we initialize from the leaf cells as:  
<center>$$ \delta(n,k) = P(X_n = x_n | z_n = k) = EL(n,k)$$</center>  
and for non-leaf cells we have:  
<center>$$ \delta(n,k) =  \Big[\prod_{v \in c(n)} \delta_{n,v}(k)\Big]\times EL(n,k)$$</center>  
The probability of the optimal state tree corresponding to the observations tree assuming root cell is noted as cell 1, is given by:
<center>$$ Z^* = \max\limits_{k}\{\delta(1,k) \pi_k \}$$</center>  
which is resulted from writing the maximization over conditional emission likelihood (EL) probabilities by factoring out the root cells as the outer maximizing step over all possible states. In this step, $Z^*$ which is the hidden state tree is calculated. After estimating the most likely state for each cell, we group the cell based on their state, which will then be used to estimate the distribution parameters.

#### Baum-Welch algorithm

In order to estimate the parameters corresponding to the hidden Markov models, Baum-Welch algorithm is used which employs the EM algorithm to find the maximum likelihood of the parameters. Here, we estimate $\theta = (\pi, T, p, a, s)$ the initial and transition probability matrices, the parameters of the observation distributions, namely Bernoulli parameter, shape, and scale parameter of Gamma distribution by maximizing the probability of the observations given the parameters; in other words $\theta^* = \max\limits_{\theta} P(X, Z|\theta)$, that is maximizing the joint probabilities.

<center>$$ \xi_n(j,k) = P(z_n = j, z_{p(n)} = i | \bar{X}_1 = \bar{x}_1)$$</center>  
According to Bayes theorem, we can write the above probability in a joint form, and since $P(\bar{X}_1) = \bar{x}_1$ is the EL probabilities of the whole tree, it is just a constant coefficient and we can ignore it.  
So we could write it as:  
<center>$$ \xi_n(j,k) = \frac{\beta(n+1, k) \times T_{j,k} \times \gamma_j(n)}{MSD(n+1, k) \times \beta_{p(n),n}(j)}$$</center>  
The maximum likelihood estimation of the initial probabilities:  
<center>$$ \pi^*_k = \gamma_1(k)$$</center>  
The transition probability matrix would be estimated as:  
<center>$$ T^*_{i,j} = \frac{\sum_{n=1}^{N-1} \xi_n(i,j)}{\sum_{n=1}^{N-1} \gamma_n(i)} $$</center>

To estimate the distribution parameters, after finding the most likely state for each cell, we group them and group their observations based on their estimated state, and pass them to _StateDistribution_ class for maximum likelihood estimation. For estimating the Bernoulli parameter the estimated parameter is simply the sample mean of the observations for each state. Assuming we have $N_1$ data points estimated in state 1, the corresponding Bernoulli parameter for state 1 would be:

<center>$$ p^*_1 = \frac{\sum_{i=1}^{N_1} x_b(i)}{N_1}$$</center>  
And for Gamma distribution parameters we use a closed-form estimation based on \cite{gamma_estimation} which has been corrected for bias:  
<center>$$ a^* = \frac{N_1 \times  \sum_{i=1}^{N_1} x_g(i)}{\Big[N_1 \times \sum_{i=1}^{N_1} x_g(i) ln(x_g(i))\Big] - \sum_{i=1}^{N_1} ln(x_g(i)) \times \sum_{i=1}^{N_1} x_g(i)} $$</center>  
<center>$$ s^* = \frac{1}{N_1^2} \times \Big[N_1 \times \sum_{i=1}^{N_1} x_g(i) ln(x_g(i)) - \sum_{i=1}^{N_1} ln(x_g(i)) \times \sum_{i=1}^{N_1} x_g(i) \Big]$$</center>

### Experimental single-cell lineage data

The data includes AU565 breast cancer cell line for control condition along with 7 concentrations of 2 types of targeted or chemotherapy compounds including lapatinib and gemcitabine. A fluorescent reporter was developed to translocate between the nucleus and cytoplasm to indicate the phase of the cells. Each cell is indicated as being in G1 or S/G2 phase according to the location of the reporter. When the reporter is located in the nucleus it means the cell is in G1 and when the reporter in in the cytoplasm, the cell is passing through S/G2 phase. Each experiment lasts for 96 hours where the plates were being imaged every 30 minutes and each experiments was repeated three times. Single cells were manually tracked to collect cell fate in either phase or the amount of time it takes for each cell to pass through G1 and S/G2 phases.

#### Strategies to overcome model fitting to censored data

In single-cell experimental data collection the lineages are almost never fully observed, in other words, we experience missing data for cells with time or fate censorship. Those leaf cells living at the end of the collected lineage trees, which may have been in their G1 or S/G2 exactly when the experiment was finished, experience a right-censorship, meaning, we have missing data from the future. For a cell that was in G1 at the end of the experiment, the time that the cell transitions to S/G2 is unknown, hence the cell's G1 is censored. In a similar way, almost all of the root cells appearing at the very beginning of the experiment are left-censored. This way, given a population of cells, we identify whether any of the cell's observation are censored or not. The uncensored values are passed to the estimator while the censored values will be handled by the survivor function. 
Another similar challenge is the unobserved measurements when cells die and their descendants disappear or when a leaf cell reaches the experiment end time while it is in G1 phase such that S/G2 phase for this cell is unobserved. We simply remove those measurements that have not been observed at all.

### Computational tools to explore the model

In this part we briefly mention the measures we used to monitor the goodness of the fit of the model. 

**AIC**  
In order to find the most likely number of states corresponding to the observations, Akaike Information Criterion (AIC) is used. This could be considered as an approach for model selection that could inform us of the most likely number of states, and also it deals with the trade off between over-fitting and under-fitting [@doi:10.1006/jmps.1999.1276].
Degrees of freedom: Our model estimate a $k \times 1$ initial probability matrix, a $k \times k$ transition matrix, and a $k \times m$ matrix of state-wise parameters where $k$ is the number of states and $m$ is the number of parameters associated with observation distributions. For the phase-specific observation sets we have a total of 6 parameters including 2 Bernoulli parameters and 2 pairs of shape and scale parameters for Gamma distribution. Since the row-sums for transition and initial probability matrices must be 1, these values are not independent. 
From distribution analysis of the phase lengths, we realized the shape parameter of the Gamma distribution remains fairly constant over different conditions, while the scale parameter changes. Removing the shape parameters of G1 and S/G2 for each states, instead of $k \times (k - 1) + (k - 1) + k \times (m)$ degrees of freedom, we will have $k \times (k - 1) + (k - 1) + k \times (m-2)$.

**Wasserstein distance**  
Wasserstein or Kantorovich–Rubinstein metric, is a measure of distance between two distributions. In order to show how different the states should be to be recognized by the model, we have employed Wasserstein distance [@doi:10.1137/1118101].

### Model benchmarking

In this model, we use multivariate emission distributions to represent physical characteristics of the cells within the lineages. To create our synthetic data we considered two possible options as our set of observations throughout an experiment. First, cell fate and cell lifetime, and second, phase-specific fate and time duration, for which we used Bernoulli and Gamma distributions. For the consistency of our model, we set a specific set of parameters for those mentioned distributions in each case. The following is the collection of distribution parameters for two-state synthetic data.

#### Phase non-specific observations
The parameters are reflective of fitting the model to data of 5 nM lapatinib treatment. Figures @fig:prop4, @fig:real_5, @fig:performUncenSingle, @fig:performUncenMulti, and @fig:performCenMulti are based on these parameters.

<table id = "phasenonspecific">
    <tr>
        <th> State </th>
        <th> Bern p </th>
        <th> Shape </th>
        <th> Scale </th> 
    </tr>
    <tr>
        <td> State 1 </td>
        <td> 0.99 </td>
        <td> 8 </td>
        <td> 6 </td>
    </tr>
    <tr>
        <td> State 2 </td>
        <td> 0.75 </td>
        <td> 8 </td>
        <td> 1 </td>
    </tr>
</table>


#### Phase specific observations
In Figures @fig:censor, @fig:performSyn, @fig:uncenSingle, @fig:uncenMulti, and @fig:cenMulti the data was created based on the following parameters. These parameters are based on estimations of 5 nM lapatinib treatment.

<table id = "phasespecific">
    <tr>
        <th> State </th>
        <th> Bern G<sub>1</sub> </th>
        <th> Bern G<sub>2</sub> </th>
        <th> Shape G<sub>1</sub> </th>
        <th> Scale G<sub>1</sub> </th>
        <th> Shape G<sub>2</sub> </th>
        <th> Scale G<sub>2</sub> </th>    
    </tr>
    <tr>
        <td> State 1 </td>
        <td> 0.99 </td>
        <td> 0.95 </td>
        <td> 8 </td>
        <td> 7 </td>
        <td> 4 </td>
        <td> 2 </td>
    </tr>
    <tr>
        <td> State 2 </td>
        <td> 0.95 </td>
        <td> 0.9 </td>
        <td> 6</td>
        <td> 4 </td>
        <td> 3 </td>
        <td> 5 </td>
    </tr>
</table>


#### Distant emissions

To create synthetic data with subpopulations of varying dissimilarity (Fig. @fig:wass), we use the phase-specific parameters set except that the values for the G1 phase Gamma shape parameter for state 1 is varied between $[4, 12]$. This results in an increase in the Wassertein distance between the two cell states, allowing us to measure state assignment accuracy for different dissimilarity amounts between the two states.  
Likewise, for Figures @fig:wass1 and @fig:wass2, we used phase non-specific parameters and varied G1 phase Gamma scale parameter between $[1, 8]$ for state 1.

#### Emissions for AIC figures

Figure @fig:sAIC uses a unique set of values for the emissions matrix in order to simulate varying states for the AIC calculation. 

<style>

    #fig7param {
    border-collapse: collapse;
    }

    #fig7param td {
        text-align: center;
    }

    #fig7param th, #fig7param td {
    padding: 5px;
    height:20px;
    width: 120px;
    }
    #fig7param th{
        border-bottom: 1px solid black;
        background-color: #f2f2f2;
    }

    #fig7param tr:nth-child(odd){background-color: #f2f2f2}   
</style>

<table id = "fig7param">
    <tr>
        <th> State </th>
        <th> Bern G<sub>1</sub> </th>
        <th> Bern G<sub>2</sub> </th>
        <th> Shape G<sub>1</sub> </th>
        <th> Scale G<sub>1</sub> </th>
        <th> Shape G<sub>2</sub> </th>
        <th> Scale G<sub>2</sub> </th>    
    </tr>
    <tr>
        <td> State 1 </td>
        <td> 0.99 </td>
        <td> 0.9 </td>
        <td> 10 </td>
        <td> 2 </td>
        <td> 10 </td>
        <td> 2 </td>
    </tr>
    <tr>
        <td> State 2 </td>
        <td> 0.9 </td>
        <td> 0.9 </td>
        <td> 20 </td>
        <td> 3 </td>
        <td> 20 </td>
        <td> 3 </td>
    </tr>
    <tr>
        <td> State 3 </td>
        <td> 0.85 </td>
        <td> 0.9 </td>
        <td> 30 </td>
        <td> 4 </td>
        <td> 30 </td>
        <td> 4 </td>
    </tr>
    <tr>
        <td> State 4 </td>
        <td> 0.8 </td>
        <td> 0.9 </td>
        <td> 40</td>
        <td> 4 </td>
        <td> 40 </td>
        <td> 5 </td>
    </tr>
</table>
 